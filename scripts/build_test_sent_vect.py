# -*- coding: utf-8 -*-
# --------------------------------------------------------------------
# MappSent: Built test sentence embedding vectors 					 -
#                                                                    -
# Author  : Amir HAZEM                                               -     
# Created : 13/02/2017                                               -  
# Updated : 15/11/2017                                               -
#                                                                    -  
# --------------------------------------------------------------------

from __future__ import division
import argparse
import numpy as np
import sys
import unicodecsv
import math
import os
import re
from nltk.corpus import wordnet as wn
from operator import add



# fix random seed for reproducibility
seed = 7
np.random.seed(seed)


#-----------------------------------------------------------------------------------------
# Params
#-----------------------------------------------------------------------------------------
model=str(sys.argv[1])		# sg for skipgram and cbow for CBOW model
dim=int(sys.argv[2])		# dimension ize of word embeddings 100 300 500 ...
w=int(sys.argv[3])			# window size for embedding training 5 7 10 ...
len_=float(sys.argv[4])		# length of tokens in terms on number of caracters 1 2 3 ...
semeval=str(sys.argv[5])    # 2016 for the 2016 edition of Semeval  data and 2017 for 2017 data
dev_or_test=str(sys.argv[6])# dev for using the development set and test for using the test set
#----------------------------------------------------------------------------------------


#-----------------------------------------------------------------------------------------
# Variables
#-----------------------------------------------------------------------------------------
# Data Location
path_data="../data/train/"
# path stopwords
path_stopwords=path_data+"stopwords_en.txt"
# path word embedding model generated by gensim toolkit
gensimvec_dir="../model/embeddings/gensim_vec/"
word_embedding_model="gensim_w2v_train_w"+str(w)+"_vec"+str(dim)+"_min5_"+model+"_lem.txt"
# unigram file location
fname_unigram=path_data+"unigram_lem.txt"
# Train 2016 and 2017 
path_data_train="../data/train/lempostag"
input_file=path_data_train+"/train_Task3_CQA_part1_lem_postag.txt"

if dev_or_test=="dev": # the development set is the same for 2016 and 2017
	path_data_eval="../data/dev/lempostag"
	input_test_file=path_data_eval+"/dev_Task3_CQA_lem_postag.txt"
else:
	path_data_eval="../data/test/lempostag"	
	if 	semeval=="2016":
		# Test 2016 -------------------------------------------------------------------
		input_test_file=path_data_eval+"/test2016_Task3_CQA_lem_postag.txt"
	else:
		# Test 2017-----------------------------------------------------------------------------
		input_test_file=path_data_eval+"/test2017_Task3_CQA_lem_postag.txt"	


unigram={}
stopwords={}
vect_sent_src={}
vect_sent_tgt={}
embedding_index={}
sum_vect_embedding_sent={}
sum_vect_embedding_sent_ori={}
sum_vect_embedding_sent_rel={}
sum_vect_embedding_sent_ori["word_ori"]=np.zeros(dim,float)
sum_vect_embedding_sent_rel["word_rel"]=np.zeros(dim,float)
#-----------------------------------------------------------------------------------------




def is_noun(tag):
    return tag in ['NN', 'NNS', 'NNP', 'NNPS']

def is_verb(tag):
    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']

def is_adverb(tag):
    return tag in ['RB', 'RBR', 'RBS']

def is_adjective(tag):
    return tag in ['JJ', 'JJR', 'JJS']

def is_wh(tag):
    return tag in ['WDT', 'WP', 'WP$', 'WRB']
	
	#WDT 	Wh-determiner
 	#WP 		Wh-pronoun
 	#WP$ 	Possessive wh-pronoun
 	#WRB 	Wh    

def penn_to_wn(tag):
	tag=tag.upper()
    	if is_adjective(tag):
        	return wn.ADJ
    	elif is_noun(tag):
        	return wn.NOUN
    	elif is_adverb(tag):
        	return wn.ADV
    	elif is_verb(tag):
        	return wn.VERB
        elif is_wh(tag):
        	return "h"	
    	return unicode(wn.ADV)


# -------------------------------------------------------------
# load_stop_words: Load stopwords
# -------------------------------------------------------------
def load_stop_words(stopwords):
	# load English stopwords
	f = open(path_stopwords,'r')
	for line in f:
		str_ =line.split()
		stopwords[str_[0]]=str((str_[0].strip()).lower())
	f.close()
	return stopwords

# -------------------------------------------------------------
# load_unigrams: Load unigram words and counts
# -------------------------------------------------------------

def load_unigrams(unigram):
	cpt=0
	total_unigram=0
	f = open(fname_unigram,'r')
	for line in f:
		a =line.split(' ')
		if a[0] !='' and a[0]!=' ' and int(a[1])>0:
			if unigram.has_key(a[0]): 
				unigram[a[0].strip()]+=int(a[1])
			else:
				unigram[a[0].strip()]=int(a[1])
				cpt+=1
			total_unigram+=int(a[1])
	f.close()
	return unigram


# -------------------------------------------------------------
# load_word_embeddings: Load word embeddings
# -------------------------------------------------------------
def load_word_embeddings(embedding_index):
	f=open(os.path.join(gensimvec_dir,word_embedding_model))
	for line in f:
		values=line.split()
		word=values[0]
		coefs=np.asarray(values[1:],dtype='float32')
		embedding_index[word]=coefs
	f.close()

	#print ('Found %s word vectors with dimension size of %s' % (len(embedding_index), len(coefs)))
	return embedding_index
	


# ------------------------------------------------------------
# save_source_sentences: Save source sentences 
# ------------------------------------------------------------
def save_source_sentences(vect_sent_src):
	fname_res="../exp/vectsent/vect_sent_src_train.txt"
	with  open(fname_res,'a') as fi:
		for sent in vect_sent_src:
			a2=str(vect_sent_src[sent]).split('[')
			b=a2[1].split(']')
			a=re.split(',*\s+',b[0])
			ch=""
			for i in a:
				if i.strip()!= "":
					ch=ch+" "+ str(float(i.strip()))

			fi.write(sent+" "+str(ch)+"\n")   


# ------------------------------------------------------------
# save_target_sentences: Save target sentences 
# ------------------------------------------------------------
def save_target_sentences(vect_sent_tgt):
	fname_res="../exp/vectsent/vect_sent_src_train.txt"

	with  open(fname_res,'a') as fi:
		for sent in vect_sent_tgt:
			a2=str(vect_sent_tgt[sent]).split('[')
			b=a2[1].split(']')
			a=re.split(',*\s+',b[0])
			ch=""
			for i in a:
				if i.strip()!= "":
					ch=ch+" "+ str(float(i.strip()))

			fi.write(sent+" "+str(ch)+"\n")   

# ------------------------------------------------------------
# parse_test: Parse data and build sentence embedding vectors 
# ------------------------------------------------------------
def parse_test():
	cpt=0
	rank=0
	
	reader = unicodecsv.reader(open(input_test_file),delimiter='\t')
	next(reader)
	for line in reader:
		
		cpt+=1	
		rank+=1
		
		if rank==11:
			rank=1
		#----------------- Head file :
		# 0  RELQ_ID	
		# 1  RELQ_CATEGORY	
		# 2  RelQSubject	
		# 3  RelQBody	
		# 4  RELQ_USERID	
		# 5  RELQ_USERNAME	
		# 6  RELQ_DATE	
		# 7  RELC_ID	
		# 8  RelCText	
		# 9  RELC_RELEVANCE2RELQ	
		# 10 RELC_USERID	
		# 11 RELC_USERNAME	
		# 12 RELC_DATE

	
		ORGQ_ID=line[0]
		OrgQSubject=line[1]	
		OrgQBody=line[2]
		RELQ_RELEVANCE2ORGQ=line[7]
		RELQ_ID=line[3]
		RELQ_CATEGORY=line[4]  
		RelQSubject=line[5]    
		RelQBody=line[6]       
		RELQ_USERNAME=line[9]	


		ori_q_sub=OrgQSubject
		ori_q_body=OrgQSubject+" "+OrgQBody #
		rel_q_sub=RelQSubject	
		rel_q_body=RelQSubject + " "+ RelQBody #


		if RELQ_RELEVANCE2ORGQ=="PerfectMatch" or RELQ_RELEVANCE2ORGQ=="Relevant":
			label=1
		else:
			label=0	


		ori_q_sub=ori_q_sub.lower()
		ori_q_body=ori_q_body.lower()
		rel_q=rel_q_body.lower()

		ori_q_split_sub=re.sub('\W+'," ", ori_q_sub).split()		
		ori_q_split_body=re.sub('\W+'," ", ori_q_body).split()		
		rel_q_split=re.sub('\W+'," ", rel_q).split()			
				
		
		if rank==1:
			
			cpt_ori=1

			for w_q in ori_q_split_body:
				w_q=w_q.split('_')
				word1=w_q[0]
				cpt+=1

				if (len(w_q)>1  ):
					tag=penn_to_wn(w_q[1])
				
				if embedding_index.has_key(word1) and len(word1)>len_ and not stopwords.has_key(word1) and (tag=='n' or tag=='a' or tag=='v') and unigram.has_key(word1) and unigram[word1]>0:
					
					cpt_ori+=1
					sum_vect_embedding_sent_ori["word_ori"]=map(add, sum_vect_embedding_sent_ori["word_ori"] ,embedding_index[word1])
					
			cpt_rel=1		
						
			for w_c in rel_q_split:
				w_c=w_c.split('_')
				word2=w_c[0]
			
				if (len(w_c)>1  ):
					tag=penn_to_wn(w_c[1])
					
				if embedding_index.has_key(word2) and len(word2)>len_ and not stopwords.has_key(word2) and (tag=='n' or tag=='a' or tag=='v') and unigram.has_key(word2) and unigram[word2]>0:
					cpt_rel+=1
					sum_vect_embedding_sent_rel["word_rel"]=map(add, sum_vect_embedding_sent_rel["word_rel"],embedding_index[word2])
					
		
			# Normalize by sentence length (optional)
			#sum_vect_embedding_sent_ori["word_ori"] = [x / cpt_ori for x in sum_vect_embedding_sent_ori["word_ori"]]
			#sum_vect_embedding_sent_rel["word_rel"] = [x / cpt_rel for x in sum_vect_embedding_sent_rel["word_rel"]]

			# store 
			vect_sent_src[ORGQ_ID]=sum_vect_embedding_sent_ori["word_ori"]
			vect_sent_tgt[RELQ_ID]=sum_vect_embedding_sent_rel["word_rel"]
			sum_vect_embedding_sent_ori["word_ori"]=np.zeros(dim,float)
			sum_vect_embedding_sent_rel["word_rel"]=np.zeros(dim,float)																
			

#----------------------------------------- 
# Main 
#-----------------------------------------

if __name__=='__main__':

	# load unigram tokens
	unigram=load_unigrams(unigram)
	
	# load stopwords 
	stopwords=load_stop_words(stopwords)
	
	# load word embedding vectors
	embedding_index=load_word_embeddings(embedding_index)

	# compute sentence embeddings of the training data set and produce the seed dictionary needed for mapping
	parse_test()
	# save test sentence embedings
	save_source_sentences(vect_sent_src)
	save_target_sentences(vect_sent_tgt)

